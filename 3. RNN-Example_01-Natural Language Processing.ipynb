{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h1><center>Recurrent Neural Network (using Dataset = imdb)</h1>\n",
    "<b><h2><center>Natural Language Processing - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is about Sentimental Analysis which is a basic form of Natural Language Processing. In this tutorial, the dataset consists of movie reviews which will be calssified as positive or negative and the Neural Network will be trained to do this classification.\n",
    "Neural Networks are compatible with numbers instead of texts but the dataset used in this tutorial contains texts. Therefore, to be compatible with Neural Networks, the text data will be converted into numbers.\n",
    "The texts in the dataset is composed of different lengths. Hence, the type of Neural Network that can work on both long and short sequences of text data will be used in this tutorial.\n",
    "The pre-requisites for this tutorial include basic know ledge of Linear Algebra, Machine Learning and Classification, Recurrent Neural Networks and Layers, Python programming language and Jupyter Notebook editor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform Sentimental Analysis from a dataset composed of text sequences, foloowing steps need to be followed:\n",
    "-  Conversion of words into integer numbers also called tokens which serves as indices to the entire vocabulary list.\n",
    "-  Conversion of tokens into real-valued vectors called embeddings, whose mapping will be trained along with the neural network.\n",
    "-  Feeding the embedding vectors into the Recurrent Neural Network (RNN) which will take sequences of arbitrary length as input and will show a summary about the input as its output.\n",
    "-  The Sigmoid-function is then applied to the Neural Network output to get the values between 0.0 and 1.0. The Value 0.0 will refer to the negative sentiment and value 1.0 will refer to the positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the libraries that will be used for the entire tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set used for the tutorial is composed of imdb movie reviews which is to be classified as either positive or negative sentiment. That means the dataset has 2 different classes. The dataset can be downloaded and loaded automatically or can also be loaded manually by downloading the dataset from an online source.\n",
    "The dataset is composed of 50,000 reviews of movies from IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "imdb.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Loading Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already divided into 2 sets; the training set and the testing set each containing 25,000 reviews. The datasets are then loaded into variable separately as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text_for_training, outputs_for_training = imdb.load_data(train=True)\n",
    "dataset_text_for_testing, outputs_for_testing = imdb.load_data(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the Sizes of Training and Testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Dataset:  25000\n",
      "Size of Testing Dataset :   25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Training Dataset: \", len(dataset_text_for_training))\n",
    "print(\"Size of Testing Dataset :  \", len(dataset_text_for_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Training and Testing Dataset for some uses in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text_in_total = dataset_text_for_training + dataset_text_for_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks does not work with text data. Therefore, the texts in the dataset will be converted into integers also called tokens so that it can be fed to the the Neural Network later.\n",
    "The Tokenizer can be instructed to use a particular number of frequently used or popular words. In this tutorial, the tokenizer is instructed to use 10,000 most popular words from the dataset as specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_number_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=take_number_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer is then fitted to the dataset where all the text is scanned and is converted to lower case along with removing any unwanted characters like punctuations.\n",
    "The Tokenizer is fit on the entire dataset including the training and testing sets. The Tokenizer will only build the vocabulary of all the unique words. The model will be trained only on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(data_text_in_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer then converts the texts in the training and testing sets into intergers called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_training = tokenizer.texts_to_sequences(dataset_text_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_testing = tokenizer.texts_to_sequences(dataset_text_for_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Recurrent Neural Networks can take the data of arbitrary lengths for the input but the text sequences should have the same length to use a whole batch of data.\n",
    "One of the solutions to do that is using a sequence-length that covers most of the data sequences. The longer sequences will then be truncated and the shorter ones will be padded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of tokens is calculated for each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_number_totals = np.array([len(tokens) for tokens in tokens_training + tokens_testing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of tokens in each sequence that will be allowed is set to the average plus two standard deviations that will cover around 95% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_sequence_length = int(np.mean(tokens_number_totals) + 2 * np.std(tokens_number_totals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rest of the data sequences, padding (adding zeros to achieve the maximum sequence length allowed) or truncating (throwing away part of the text to achieve the maximum sequence length allowed) is done. It has to be specified whether the truncating or padding should be 'pre' or 'post'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_padded_tokens = pad_sequences(tokens_training, maxlen=maximum_sequence_length, padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_padded_tokens = pad_sequences(tokens_testing, maxlen=maximum_sequence_length, padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Tokenizer Inverse Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras does not have any implementation of mapping the integer token back to words, therefore, it is done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_mapping_indices = tokenizer.word_index\n",
    "inverse_map = dict(zip(inverse_mapping_indices.values(), inverse_mapping_indices.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Funtion to Convert Tokens into Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_tokens):\n",
    "    output_words = [inverse_map[tokens] for tokens in input_tokens if input_tokens !=0]\n",
    "    converted_string = \" \".join(output_words)\n",
    "    return converted_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, below is an example of the original text from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When I first read Armistead Maupins story I was taken in by the human drama displayed by Gabriel No one and those he cares about and loves. That being said, we have now been given the film version of an excellent story and are expected to see past the gloss of Hollywood...<br /><br />Writer Armistead Maupin and director Patrick Stettner have truly succeeded! <br /><br />With just the right amount of restraint Robin Williams captures the fragile essence of Gabriel and lets us see his struggle with issues of trust both in his personnel life(Jess) and the world around him(Donna).<br /><br />As we are introduced to the players in this drama we are reminded that nothing is ever as it seems and that the smallest event can change our lives irrevocably. The request to review a book written by a young man turns into a life changing event that helps Gabriel find the strength within himself to carry on and move forward.<br /><br />It\\'s to bad that most people will avoid this film. I only say that because the average American will probably think \"Robin Williams in a serious role? That didn\\'t work before!\" PLEASE GIVE THIS MOVIE A CHANCE! Robin Williams touches the darkness we all must find and go through in ourselves to be better people. Like his movie One Hour Photo he has stepped up as an actor and made another quality piece of art.<br /><br />Oh and before I forget, I believe Bobby Cannavale as Jess steals every scene he is in. He has the 1940\\'s leading man looks and screen presence. It\\'s this hacks opinion he could carry his own movie right now!!<br /><br />S~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_text_for_training[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the array of tokens of the corresponding dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  50,   10,   86,  339,   64,   10,   13,  607,    8,   31,    1,\n",
       "        395,  449, 4380,   31, 5078,   54,   27,    2,  143,   28, 2150,\n",
       "         42,    2, 1358,   12,  109,  301,   73,   25,  146,   75,  358,\n",
       "          1,   19,  318,    4,   32,  320,   64,    2,   23,  862,    5,\n",
       "         63,  509,    1,    4,  369,    7,    7,  563,    2,  164, 2372,\n",
       "         25,  371, 4163,    7,    7,   16,   39,    1,  203, 1120,    4,\n",
       "       8163, 1943, 1858, 2446,    1, 7019, 3258,    4, 5078,    2, 1573,\n",
       "        176,   63,   24, 1708,   16, 1262,    4, 1740,  195,    8,   24,\n",
       "        114, 6641,    2,    1,  181,  183,   87, 5050,    7,    7,   14,\n",
       "         73,   23, 1705,    5,    1, 1887,    8,   11,  449,   73,   23,\n",
       "       1568,   12,  161,    6,  123,   14,    9,  184,    2,   12,    1,\n",
       "       8927, 1560,   67,  665,  260,  468,    1, 7954,    5,  715,    3,\n",
       "        275,  407,   31,    3,  186,  128,  511,   82,    3,  114, 2538,\n",
       "       1560,   12, 1526, 5078,  165,    1, 2095,  748,  313,    5, 1672,\n",
       "         20,    2,  831,  929,    7,    7,   44,    5,   74,   12,   88,\n",
       "         83,   80,  793,   11,   19,   10,   61,  131,   12,   84,    1,\n",
       "        848,  283,   80,  239,  101, 1943, 1858,    8,    3,  600,  214,\n",
       "         12,  154,  158,  159,  591,  197,   11,   17,    3,  574, 1943,\n",
       "       1858, 2531,    1, 2367,   73,   29,  206,  165,    2,  139,  140,\n",
       "          8, 3158,    5,   26,  126,   83,   37,   24,   17,   27,  527,\n",
       "       4806,   28,   45, 9313,   53,   14,   32,  291,    2,   90,  157,\n",
       "        481,  412,    4,  503,    7,    7,  429,    2,  159,   10,  834,\n",
       "         10,  262, 3177,   14, 6641, 2261,  170,  129,   28,    6,    8,\n",
       "         28,   45,    1, 8231,  993,  128,  285,    2,  258, 1308,   44,\n",
       "         11,  664,   28,   98, 1672,   24,  199,   17,  203,  146,    7,\n",
       "          7,  580])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokens_training[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is what the text looks like when the tokens are converted back to words. The original and translated texts can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"when i first read story i was taken in by the human drama displayed by gabriel no one and those he cares about and loves that being said we have now been given the film version of an excellent story and are expected to see past the of hollywood br br writer and director patrick have truly succeeded br br with just the right amount of restraint robin williams captures the fragile essence of gabriel and lets us see his struggle with issues of trust both in his life jess and the world around him donna br br as we are introduced to the players in this drama we are reminded that nothing is ever as it seems and that the smallest event can change our lives the request to review a book written by a young man turns into a life changing event that helps gabriel find the strength within himself to carry on and move forward br br it's to bad that most people will avoid this film i only say that because the average american will probably think robin williams in a serious role that didn't work before please give this movie a chance robin williams touches the darkness we all must find and go through in ourselves to be better people like his movie one hour photo he has stepped up as an actor and made another quality piece of art br br oh and before i forget i believe bobby as jess steals every scene he is in he has the 1940's leading man looks and screen presence it's this opinion he could carry his own movie right now br br s\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(tokens_training[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Creating Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Recurrent Neural Network is created using the Keras API for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first RNN layer is the Embedding Layer that converts integers into vectors of values. Embedding Layer makes the integer tokens to take on values between 0 and 10000 for a vocabulary of 10000 words. This is because the RNN cannot work on a wide range of values.\n",
    "The size of the embedding vector has to be defined for each integer token. For Sentimental Analysis, smaller values for the embedding vector works better. In this tutorial, it is set to 8. Now each token will be converted into a vector of length 8. Embedding Vector values lies roughly between -1.0 and 1.0.\n",
    "There are a few other things that needs to be specified for adding an Embedding Layer in the model that include:\n",
    "-  total number of words in the vocabulary.\n",
    "-  maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_for_embedding = 8\n",
    "selected_model.add(Embedding(input_dim=take_number_words,\n",
    "                             output_dim=size_for_embedding,\n",
    "                             input_length=maximum_sequence_length,\n",
    "                             name='layer_embedding'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the Embedding Layer, the Gated Recurrent Unit (GRU) Layers are added along with their number of outputs. The output of 1st GRU layer is fed as an input to the 2nd GRU Layer. In the same way, the output of 2nd GRU layer is fed as an input to the 3rd GRU Layer.\n",
    "For the first two GRU Layers, sequences of data needs to be returned. This returened sequences will also be used as an input by the next GRU Layer whereas the last GRU Layer only needs to give the final output which is fed to the Dense Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model.add(GRU(units=16, return_sequences=True))\n",
    "selected_model.add(GRU(units=8, return_sequences=True))\n",
    "selected_model.add(GRU(units=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer of the model is the Fully Connected Dense Layer that will compute the final output as a value between 0.0 and 1.0 which is used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the Keras model after adding the Adam Optimizer with a certain learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model.compile(loss='binary_crossentropy',\n",
    "                       optimizer=Adam(lr=1e-3),\n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below command is used to check out the model summary and gives insights into the compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 544, 8)            80000     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 16)          1200      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 8)           600       \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 81,961\n",
      "Trainable params: 81,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "selected_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Training the Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then trained using the padded sequences of the training set. 5% of the training dataset is used as validation set to get a rough estimate of how well the model is doing or perhaps it is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================]23750/23750 [==============================] - 450s 19ms/step - loss: 0.6570 - acc: 0.5883 - val_loss: 0.3931 - val_acc: 0.8936\n",
      "\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================]23750/23750 [==============================] - 442s 19ms/step - loss: 0.4654 - acc: 0.7933 - val_loss: 0.3243 - val_acc: 0.8840\n",
      "\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================]23750/23750 [==============================] - 446s 19ms/step - loss: 0.3464 - acc: 0.8584 - val_loss: 0.7269 - val_acc: 0.6728\n",
      "\n",
      "Wall time: 22min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x23db1f79940>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "selected_model.fit(training_data_padded_tokens,\n",
    "                   outputs_for_training,\n",
    "                   validation_split=0.05,\n",
    "                   epochs=3,\n",
    "                   batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Performance on Test-Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, its accuracy on the test set is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================]25000/25000 [==============================] - 147s 6ms/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation = selected_model.evaluate(testing_data_padded_tokens, outputs_for_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.18%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {0:.2%}\".format(evaluation[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Mis-Classified Text Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show a Mis-Classified Text example, the predicted sentiment for the first 1000 texts in the testing set is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_set = selected_model.predict(x=testing_data_padded_tokens[0:1000])\n",
    "predicted_test_set = predicted_test_set.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted values range between 0.0 and 1.0. A threshold/cutoff is used to predict the class as either 1.0 or 0.0. The values above 0.5 are taken as 1.0 and the below ones are taken as 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_set_class = np.array([1.0 if p>0.5 else 0.0 for p in predicted_test_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true classes of the first 1000 texts in the testing set are also need to compare them with the predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test_set_class = np.array(outputs_for_testing[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the predicted and true Classes of the test set, the indices of all the incorrect classified texts can be collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = np.where(predicted_test_set_class != true_test_set_class)\n",
    "incorrect = incorrect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = incorrect[0]\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a mis-classified text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test_set_class[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_test_set_class[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>New Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New texts can be added to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewText1 = \"This movie is great! I like it because it is very good!\"\n",
    "NewText2 = \"Fantastic movie!\"\n",
    "NewText3 = \"Maybe I do not like this movie.\"\n",
    "NewText4 = \"Meh ...\"\n",
    "NewText5 = \"If I were a drunk teenager then this movie might be good.\"\n",
    "NewText6 = \"Worst movie!\"\n",
    "NewText7 = \"Not a nice movie!\"\n",
    "NewText8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "Texts = [NewText1, NewText2, NewText3, NewText4, NewText5, NewText6, NewText7, NewText8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting New Texts into Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_NewText = tokenizer.texts_to_sequences(Texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 17, 6, 78, 10, 37, 9, 84, 9, 6, 52, 49],\n",
       " [799, 17],\n",
       " [273, 10, 77, 21, 37, 11, 17],\n",
       " [],\n",
       " [43, 10, 70, 3, 1942, 2188, 91, 11, 17, 233, 26, 49],\n",
       " [246, 17],\n",
       " [21, 3, 331, 17],\n",
       " [11, 17, 62, 1692, 67, 10, 76, 56, 290, 141, 591]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_NewText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding Token Sequences of NewTexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_padded_NewText = pad_sequences(tokens_NewText, maxlen = maximum_sequence_length, padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   6,  52,  49],\n",
       "       [  0,   0,   0, ...,   0, 799,  17],\n",
       "       [  0,   0,   0, ...,  37,  11,  17],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   0, 246,  17],\n",
       "       [  0,   0,   0, ...,   3, 331,  17],\n",
       "       [  0,   0,   0, ..., 290, 141, 591]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_padded_NewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 544)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_padded_NewText.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction on New Texts Using the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92998534],\n",
       "       [0.7742285 ],\n",
       "       [0.6559391 ],\n",
       "       [0.5634046 ],\n",
       "       [0.75763464],\n",
       "       [0.33406493],\n",
       "       [0.76830375],\n",
       "       [0.48407638]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_model.predict(tokens_padded_NewText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Getting Embedded Layer and its Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check out the weights in the embedding layer, get the embedding layer from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding_Layer = selected_model.get_layer('layer_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get weights of the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding_Weights = selected_model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the shape of Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding_Weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the weights for a few examples, get tokens of a few words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_for_word_nice = tokenizer.word_index['nice']\n",
    "token_for_word_nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1322"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_for_word_terrific = tokenizer.word_index['terrific']\n",
    "token_for_word_terrific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Embedding weights of the corresponding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3455799 ,  0.31407207, -0.09928934,  0.15568393,  0.13026911,\n",
       "        0.15672676,  0.9624171 ,  0.21042493], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding_Weights[token_for_word_nice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7768304 , 0.5274931 , 0.34701896, 1.1513089 , 0.8382184 ,\n",
       "       0.83672684, 0.3298065 , 0.5625317 ], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding_Weights[token_for_word_terrific]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h1><center>!---The End---!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
